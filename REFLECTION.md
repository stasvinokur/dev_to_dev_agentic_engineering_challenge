# Рефлексия: SonarGatekeeper MCP

## Что такое SonarGatekeeper

MCP-сервер, который помогает разработчику пройти путь от «quality gate упал» до готового PR за 5-10 минут. Pipeline из 5 агентов автоматически собирает данные из SonarQube, группирует issues по приоритету, генерирует минимальные патчи, верифицирует их и формирует описание PR. Всё работает локально.

## Процесс разработки с AI-агентами

Весь проект разработан с помощью AI-агента в парадигме «AI пишет код, человек ревьюит и направляет».

**Управление задачами**: использовали Backlog.md CLI — 16 задач от архитектуры до финальной полировки. Каждая задача с acceptance criteria, implementation plan, notes. Это позволило не терять контекст между сессиями.

**Workflow**: человек формулирует задачу → AI-агент исследует кодовую базу → предлагает план → после одобрения реализует → запускает тесты → коммитит. Человек контролирует направление, AI делает рутину.

**Инструменты**: AI coding agent, Bun runtime, Docker Compose, SonarQube, Ollama (qwen2.5-coder:1.5b), Langfuse для трассировки.

## Что сработало хорошо

**Двухслойная архитектура**. Разделение на MCP-инструменты (чистый доступ к данным) и Agent Pipeline (LLM-логика) — ключевое решение. Инструменты можно использовать отдельно от pipeline, а pipeline не зависит от MCP-транспорта. CLI и MCP-клиенты используют один и тот же код.

**LLM Skills в Markdown**. Каждый агент (Triage, Fix, Reporter) получает системный промпт из `.md` файла. Это позволяет менять поведение агента без правки TypeScript — просто отредактировав Markdown. Например, перевод вывода на русский потребовал добавления одной строки в каждый skill-файл.

**Retry-логика**. Verifier проверяет патчи через `bun test` + `oxlint` + `oxfmt`. Если проверка не пройдена, Fix получает feedback и пробует снова (до 2 попыток). Это значительно повышает качество патчей.

**Langfuse трассировка**. Каждый LLM-вызов трассируется с входными/выходными данными. Это критически важно для отладки — видно что именно получил LLM и что ответил.

**Backlog-driven разработка**. 16 задач с чёткими acceptance criteria позволили работать итеративно. AI-агент не терял контекст между сессиями благодаря задачам с implementation notes.

## Что не сработало / трудности

**LLM не всегда следует инструкциям по языку**. Несмотря на явную инструкцию «отвечай на русском» в skill-файлах, Qwen3-Coder иногда генерирует категории триажа на английском. Проблема в том, что модель обучена преимущественно на английском коде, и технические описания «скатываются» в английский.

**Сложность отладки agent pipeline**. Когда pipeline из 5 агентов работает последовательно, ошибка на раннем этапе (например, неправильный парсинг JSON от Triage) каскадно ломает всё дальше. Без Langfuse отлаживать это было бы крайне сложно.

**Патчи не всегда корректны**. LLM генерирует `original` / `replacement` блоки, но иногда `original` не совпадает с реальным кодом (лишний пробел, другой отступ). Мы решили это через retry с feedback от Verifier, но 100% точности добиться не удалось.

**Docker + MCP stdio**. Изначально MCP-сервер общался только через stdio, что усложняло Docker-контейнеризацию. Решили добавлением HTTP Streamable transport (`src/serve.ts`) — теперь `docker run IMAGE serve` поднимает полноценный HTTP-сервер на порту 8000 с `/mcp` и `/health`, а stdio-режим сохранён для MCP-клиентов.

## Ключевые архитектурные решения

1. **Vercel AI SDK** вместо прямых HTTP-вызовов к Ollama — единый интерфейс для любого LLM-провайдера, structured output через Zod-схемы.

2. **Zod-валидация** на всех границах: MCP tool parameters, LLM structured output, конфигурация. Ошибки ловятся рано с понятными сообщениями.

3. **Pipeline decoupled от MCP** — `runPipeline()` вызывается и из MCP tool, и из CLI. Это позволило добавить CLI за 1 задачу без рефакторинга pipeline.

4. **Collector без LLM, Verifier без LLM** — не всё требует LLM. Сбор данных из SonarQube и запуск проверок — чисто программная логика. Это экономит время и токены.

5. **Локальный стек** — SonarQube, Ollama, Langfuse, PostgreSQL, ClickHouse, Redis, MinIO — всё в Docker Compose. Ни одного облачного вызова. Код не покидает машину разработчика.

## Выводы

- **AI-агенты эффективны для рутины**, но требуют жёсткого контроля: чёткие промпты, валидация выходов, retry при ошибках.
- **Structured output (Zod) — must have** для agent pipeline. Без него парсинг свободного текста от LLM — источник 80% багов.
- **Трассировка (Langfuse) — не опция, а необходимость** при работе с LLM. Без неё отладка pipeline превращается в гадание.
- **Backlog-driven workflow с AI** работает лучше, чем «просто попросить AI написать код». Задачи с acceptance criteria — контракт между человеком и AI.
